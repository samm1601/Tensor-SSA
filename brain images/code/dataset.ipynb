{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1147c623",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Please use HDF reader for matlab v7.3 files",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19932\\4000215008.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Load the provided dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/Mrlaptop/Downloads/Brain_Data_Mat_Format/Brain_Data_Mat_Format/008-01reflectance.mat\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Inspect the keys to understand the structure of the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[1;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'variable_names'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m         \u001b[0mMR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py\u001b[0m in \u001b[0;36mmat_reader_factory\u001b[1;34m(file_name, appendmat, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mMatFile5Reader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_opened\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmjv\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Please use HDF reader for matlab v7.3 files'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Did not recognize version %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmjv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Please use HDF reader for matlab v7.3 files"
     ]
    }
   ],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tSVD(tensor, rank):\n",
    "    \"\"\" Tensor Singular Value Decomposition \"\"\"\n",
    "    unfold_tensor = tensor.reshape(tensor.shape[0], -1)\n",
    "    U, S, VT = np.linalg.svd(unfold_tensor, full_matrices=False)\n",
    "    U = U[:, :rank]\n",
    "    S = np.diag(S)[:rank, :rank]\n",
    "    VT = VT[:rank, :]\n",
    "    return U, S, VT\n",
    "\n",
    "def tProduct(U, S, VT):\n",
    "    \"\"\" Tensor Product \"\"\"\n",
    "    result = np.tensordot(U, S, axes=(1, 0))\n",
    "    result = np.tensordot(result, VT, axes=(1, 0))\n",
    "    return result\n",
    "\n",
    "# Load the provided dataset\n",
    "data = sio.loadmat(\"C:/Users/Mrlaptop/Downloads/Brain_Data_Mat_Format/Brain_Data_Mat_Format/008-01reflectance.mat\")\n",
    "\n",
    "# Inspect the keys to understand the structure of the data\n",
    "print(data.keys())\n",
    "\n",
    "# Assuming the dataset has similar structure\n",
    "# img_gt = data['some_key_for_ground_truth']\n",
    "# img = data['some_key_for_image']\n",
    "\n",
    "# Update this with actual keys after inspecting the structure\n",
    "img = data['reflectance']\n",
    "\n",
    "# Example placeholder for ground truth\n",
    "W, H, B = img.shape\n",
    "img_gt = np.random.randint(1, 5, (W, H))  # Assuming there are 4 classes\n",
    "\n",
    "# Parameters (Can be changed as required)\n",
    "u = 5\n",
    "w = 2 * u + 1\n",
    "w2 = w * w\n",
    "L = 49\n",
    "\n",
    "# TensorSSA\n",
    "indian_pines = np.pad(img, ((u, u), (u, u), (0, 0)), mode='symmetric')\n",
    "Id = np.zeros((L, W * H), dtype=int)\n",
    "Fea_cube = np.zeros((L, W * H, B))\n",
    "\n",
    "# Adaptive embedding\n",
    "k = 0\n",
    "for i in range(W):\n",
    "    for j in range(H):\n",
    "        i1 = i + u\n",
    "        j1 = j + u\n",
    "        k += 1\n",
    "        testcube = indian_pines[i1-u:i1+u+1, j1-u:j1+u+1, :]\n",
    "        m = testcube.reshape(w2, B)\n",
    "\n",
    "        # NED\n",
    "        center = m[(w2+1)//2, :]\n",
    "        NED = np.sqrt(np.sum(((m / np.linalg.norm(m, axis=1, keepdims=True)) - (center / np.linalg.norm(center)))**2, axis=1))\n",
    "        ind = np.argsort(NED)\n",
    "        index = ind[:L]\n",
    "        Id[:, k-1] = index\n",
    "        Fea_cube[:, k-1, :] = m[index, :]\n",
    "\n",
    "# T-SVD decomposition\n",
    "rank = min(L, B)  # Ensure the rank is appropriate for the tensor dimensions\n",
    "U, S, VT = tSVD(Fea_cube, rank)\n",
    "\n",
    "# Ensure shapes are compatible for tProduct\n",
    "print(f\"Shapes before tProduct: U={U.shape}, S={S.shape}, VT={VT.shape}\")\n",
    "assert U.shape[1] == S.shape[0]\n",
    "assert S.shape[1] == VT.shape[0]\n",
    "\n",
    "C = tProduct(U, S, VT)\n",
    "print(f\"Shape after first tProduct (C): {C.shape}\")\n",
    "\n",
    "# Perform the final tProduct\n",
    "Feacube_proc = C.reshape(L, W * H, B)\n",
    "print(f\"Shape after second tProduct (Feacube_proc): {Feacube_proc.shape}\")\n",
    "\n",
    "# SVM and RF based classification\n",
    "Labels = img_gt.reshape(W*H)\n",
    "Vectors = Feacube_proc.transpose(1, 0, 2).reshape(W*H, L*B)  # Use processed feature cube for SVM and RF\n",
    "\n",
    "class_num = np.max(img_gt) - np.min(img_gt)\n",
    "trainVectors, trainLabels, testVectors, testLabels = [], [], [], []\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "Sam = 0.02  # Training sample ratio: 0.02 for IP, 0.01 for PU and MG\n",
    "\n",
    "for k in range(1, class_num + 1):\n",
    "    index = np.where(Labels == k)[0]\n",
    "    perclass_num = len(index)\n",
    "    Vectors_perclass = Vectors[index, :]\n",
    "    c = rng.permutation(perclass_num)\n",
    "    select_train = Vectors_perclass[c[:int(np.ceil(perclass_num * Sam))], :]\n",
    "    train_index_k = index[c[:int(np.ceil(perclass_num * Sam))]]\n",
    "    trainVectors.append(select_train)\n",
    "    trainLabels.extend([k] * int(np.ceil(perclass_num * Sam)))\n",
    "\n",
    "    select_test = Vectors_perclass[c[int(np.ceil(perclass_num * Sam)):], :]\n",
    "    test_index_k = index[c[int(np.ceil(perclass_num * Sam)):]]\n",
    "    testVectors.append(select_test)\n",
    "    testLabels.extend([k] * (perclass_num - int(np.ceil(perclass_num * Sam))))\n",
    "\n",
    "trainVectors = np.vstack(trainVectors)\n",
    "testVectors = np.vstack(testVectors)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler().fit(trainVectors)\n",
    "trainVectors_scaled = scaler.transform(trainVectors)\n",
    "testVectors_scaled = scaler.transform(testVectors)\n",
    "Vectors_scaled = scaler.transform(Vectors)\n",
    "\n",
    "# SVM classification\n",
    "svm_classifier = SVC(kernel='linear', C=1)\n",
    "svm_classifier.fit(trainVectors_scaled, trainLabels)\n",
    "predicted_labels_svm = svm_classifier.predict(testVectors_scaled)\n",
    "\n",
    "# RF classification\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_classifier.fit(trainVectors_scaled, trainLabels)\n",
    "predicted_labels_rf = rf_classifier.predict(testVectors_scaled)\n",
    "\n",
    "# Evaluate the performance\n",
    "conf_matrix_svm = confusion_matrix(testLabels, predicted_labels_svm)\n",
    "accuracy_svm = accuracy_score(testLabels, predicted_labels_svm)\n",
    "kappa_svm = cohen_kappa_score(testLabels, predicted_labels_svm)\n",
    "\n",
    "conf_matrix_rf = confusion_matrix(testLabels, predicted_labels_rf)\n",
    "accuracy_rf = accuracy_score(testLabels, predicted_labels_rf)\n",
    "kappa_rf = cohen_kappa_score(testLabels, predicted_labels_rf)\n",
    "\n",
    "print(f\"SVM Confusion Matrix:\\n{conf_matrix_svm}\")\n",
    "print(f\"SVM Accuracy: {accuracy_svm}\")\n",
    "print(f\"SVM Kappa: {kappa_svm}\")\n",
    "\n",
    "print(f\"RF Confusion Matrix:\\n{conf_matrix_rf}\")\n",
    "print(f\"RF Accuracy: {accuracy_rf}\")\n",
    "print(f\"RF Kappa: {kappa_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f70cd9c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (truncated file: eof = 46974730, sblock->base_addr = 512, stored_eof = 473815497)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19932\\217564442.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:/Users/Mrlaptop/Downloads/Brain_Data_Mat_Format/Brain_Data_Mat_Format/008-01reflectance.mat\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Assuming the dataset key is 'reflectance'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Update this with the actual key after inspecting the structure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[0;32m    531\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[1;32m--> 533\u001b[1;33m                 \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\anaconda\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (truncated file: eof = 46974730, sblock->base_addr = 512, stored_eof = 473815497)"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tSVD(tensor, rank):\n",
    "    \"\"\" Tensor Singular Value Decomposition \"\"\"\n",
    "    unfold_tensor = tensor.reshape(tensor.shape[0], -1)\n",
    "    U, S, VT = np.linalg.svd(unfold_tensor, full_matrices=False)\n",
    "    U = U[:, :rank]\n",
    "    S = np.diag(S)[:rank, :rank]\n",
    "    VT = VT[:rank, :]\n",
    "    return U, S, VT\n",
    "\n",
    "def tProduct(U, S, VT):\n",
    "    \"\"\" Tensor Product \"\"\"\n",
    "    result = np.tensordot(U, S, axes=(1, 0))\n",
    "    result = np.tensordot(result, VT, axes=(1, 0))\n",
    "    return result\n",
    "\n",
    "# Load the provided dataset\n",
    "file_path = \"C:/Users/Mrlaptop/Downloads/Brain_Data_Mat_Format/Brain_Data_Mat_Format/008-01reflectance.mat\"\n",
    "\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    # Assuming the dataset key is 'reflectance'\n",
    "    # Update this with the actual key after inspecting the structure\n",
    "    img = f['reflectance'][:]\n",
    "\n",
    "# Example placeholder for ground truth\n",
    "W, H, B = img.shape\n",
    "img_gt = np.random.randint(1, 5, (W, H))  # Assuming there are 4 classes\n",
    "\n",
    "# Parameters (Can be changed as required)\n",
    "u = 5\n",
    "w = 2 * u + 1\n",
    "w2 = w * w\n",
    "L = 49\n",
    "\n",
    "# TensorSSA\n",
    "indian_pines = np.pad(img, ((u, u), (u, u), (0, 0)), mode='symmetric')\n",
    "Id = np.zeros((L, W * H), dtype=int)\n",
    "Fea_cube = np.zeros((L, W * H, B))\n",
    "\n",
    "# Adaptive embedding\n",
    "k = 0\n",
    "for i in range(W):\n",
    "    for j in range(H):\n",
    "        i1 = i + u\n",
    "        j1 = j + u\n",
    "        k += 1\n",
    "        testcube = indian_pines[i1-u:i1+u+1, j1-u:j1+u+1, :]\n",
    "        m = testcube.reshape(w2, B)\n",
    "\n",
    "        # NED\n",
    "        center = m[(w2+1)//2, :]\n",
    "        NED = np.sqrt(np.sum(((m / np.linalg.norm(m, axis=1, keepdims=True)) - (center / np.linalg.norm(center)))**2, axis=1))\n",
    "        ind = np.argsort(NED)\n",
    "        index = ind[:L]\n",
    "        Id[:, k-1] = index\n",
    "        Fea_cube[:, k-1, :] = m[index, :]\n",
    "\n",
    "# T-SVD decomposition\n",
    "rank = min(L, B)  # Ensure the rank is appropriate for the tensor dimensions\n",
    "U, S, VT = tSVD(Fea_cube, rank)\n",
    "\n",
    "# Ensure shapes are compatible for tProduct\n",
    "print(f\"Shapes before tProduct: U={U.shape}, S={S.shape}, VT={VT.shape}\")\n",
    "assert U.shape[1] == S.shape[0]\n",
    "assert S.shape[1] == VT.shape[0]\n",
    "\n",
    "C = tProduct(U, S, VT)\n",
    "print(f\"Shape after first tProduct (C): {C.shape}\")\n",
    "\n",
    "# Perform the final tProduct\n",
    "Feacube_proc = C.reshape(L, W * H, B)\n",
    "print(f\"Shape after second tProduct (Feacube_proc): {Feacube_proc.shape}\")\n",
    "\n",
    "# SVM and RF based classification\n",
    "Labels = img_gt.reshape(W*H)\n",
    "Vectors = Feacube_proc.transpose(1, 0, 2).reshape(W*H, L*B)  # Use processed feature cube for SVM and RF\n",
    "\n",
    "class_num = np.max(img_gt) - np.min(img_gt)\n",
    "trainVectors, trainLabels, testVectors, testLabels = [], [], [], []\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "Sam = 0.02  # Training sample ratio: 0.02 for IP, 0.01 for PU and MG\n",
    "\n",
    "for k in range(1, class_num + 1):\n",
    "    index = np.where(Labels == k)[0]\n",
    "    perclass_num = len(index)\n",
    "    Vectors_perclass = Vectors[index, :]\n",
    "    c = rng.permutation(perclass_num)\n",
    "    select_train = Vectors_perclass[c[:int(np.ceil(perclass_num * Sam))], :]\n",
    "    train_index_k = index[c[:int(np.ceil(perclass_num * Sam))]]\n",
    "    trainVectors.append(select_train)\n",
    "    trainLabels.extend([k] * int(np.ceil(perclass_num * Sam)))\n",
    "\n",
    "    select_test = Vectors_perclass[c[int(np.ceil(perclass_num * Sam)):], :]\n",
    "    test_index_k = index[c[int(np.ceil(perclass_num * Sam)):]]\n",
    "    testVectors.append(select_test)\n",
    "    testLabels.extend([k] * (perclass_num - int(np.ceil(perclass_num * Sam))))\n",
    "\n",
    "trainVectors = np.vstack(trainVectors)\n",
    "testVectors = np.vstack(testVectors)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler().fit(trainVectors)\n",
    "trainVectors_scaled = scaler.transform(trainVectors)\n",
    "testVectors_scaled = scaler.transform(testVectors)\n",
    "Vectors_scaled = scaler.transform(Vectors)\n",
    "\n",
    "# SVM classification\n",
    "svm_classifier = SVC(kernel='linear', C=1)\n",
    "svm_classifier.fit(trainVectors_scaled, trainLabels)\n",
    "predicted_labels_svm = svm_classifier.predict(testVectors_scaled)\n",
    "\n",
    "# RF classification\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_classifier.fit(trainVectors_scaled, trainLabels)\n",
    "predicted_labels_rf = rf_classifier.predict(testVectors_scaled)\n",
    "\n",
    "# Evaluate the performance\n",
    "conf_matrix_svm = confusion_matrix(testLabels, predicted_labels_svm)\n",
    "accuracy_svm = accuracy_score(testLabels, predicted_labels_svm)\n",
    "kappa_svm = cohen_kappa_score(testLabels, predicted_labels_svm)\n",
    "\n",
    "conf_matrix_rf = confusion_matrix(testLabels, predicted_labels_rf)\n",
    "accuracy_rf = accuracy_score(testLabels, predicted_labels_rf)\n",
    "kappa_rf = cohen_kappa_score(testLabels, predicted_labels_rf)\n",
    "\n",
    "print(f\"SVM Confusion Matrix:\\n{conf_matrix_svm}\")\n",
    "print(f\"SVM Accuracy: {accuracy_svm}\")\n",
    "print(f\"SVM Kappa: {kappa_svm}\")\n",
    "\n",
    "print(f\"RF Confusion Matrix:\\n{conf_matrix_rf}\")\n",
    "print(f\"RF Accuracy: {accuracy_rf}\")\n",
    "print(f\"RF Kappa: {kappa_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70bef8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading the file: Unable to open file (truncated file: eof = 46974730, sblock->base_addr = 512, stored_eof = 473815497)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19932\\843712506.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# Ensure that the data was loaded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[1;34m'reflectance'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reflectance'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tSVD(tensor, rank):\n",
    "    \"\"\" Tensor Singular Value Decomposition \"\"\"\n",
    "    unfold_tensor = tensor.reshape(tensor.shape[0], -1)\n",
    "    U, S, VT = np.linalg.svd(unfold_tensor, full_matrices=False)\n",
    "    U = U[:, :rank]\n",
    "    S = np.diag(S)[:rank, :rank]\n",
    "    VT = VT[:rank, :]\n",
    "    return U, S, VT\n",
    "\n",
    "def tProduct(U, S, VT):\n",
    "    \"\"\" Tensor Product \"\"\"\n",
    "    result = np.tensordot(U, S, axes=(1, 0))\n",
    "    result = np.tensordot(result, VT, axes=(1, 0))\n",
    "    return result\n",
    "\n",
    "# Load the provided dataset\n",
    "file_path =  \"C:/Users/Mrlaptop/Downloads/Brain_Data_Mat_Format/Brain_Data_Mat_Format/008-01reflectance.mat\"\n",
    "\n",
    "try:\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Inspecting the keys to understand the structure of the data\n",
    "        print(f.keys())\n",
    "        # Assuming the dataset key is 'reflectance'\n",
    "        # Update this with the actual key after inspecting the structure\n",
    "        data = {key: np.array(f[key]) for key in f.keys()}\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the file: {e}\")\n",
    "\n",
    "# Ensure that the data was loaded\n",
    "if 'reflectance' in data:\n",
    "    img = data['reflectance']\n",
    "else:\n",
    "    raise KeyError(\"The key 'reflectance' was not found in the data. Available keys are: \", data.keys())\n",
    "\n",
    "# Example placeholder for ground truth\n",
    "W, H, B = img.shape\n",
    "img_gt = np.random.randint(1, 5, (W, H))  # Assuming there are 4 classes\n",
    "\n",
    "# Parameters (Can be changed as required)\n",
    "u = 5\n",
    "w = 2 * u + 1\n",
    "w2 = w * w\n",
    "L = 49\n",
    "\n",
    "# TensorSSA\n",
    "indian_pines = np.pad(img, ((u, u), (u, u), (0, 0)), mode='symmetric')\n",
    "Id = np.zeros((L, W * H), dtype=int)\n",
    "Fea_cube = np.zeros((L, W * H, B))\n",
    "\n",
    "# Adaptive embedding\n",
    "k = 0\n",
    "for i in range(W):\n",
    "    for j in range(H):\n",
    "        i1 = i + u\n",
    "        j1 = j + u\n",
    "        k += 1\n",
    "        testcube = indian_pines[i1-u:i1+u+1, j1-u:j1+u+1, :]\n",
    "        m = testcube.reshape(w2, B)\n",
    "\n",
    "        # NED\n",
    "        center = m[(w2+1)//2, :]\n",
    "        NED = np.sqrt(np.sum(((m / np.linalg.norm(m, axis=1, keepdims=True)) - (center / np.linalg.norm(center)))**2, axis=1))\n",
    "        ind = np.argsort(NED)\n",
    "        index = ind[:L]\n",
    "        Id[:, k-1] = index\n",
    "        Fea_cube[:, k-1, :] = m[index, :]\n",
    "\n",
    "# T-SVD decomposition\n",
    "rank = min(L, B)  # Ensure the rank is appropriate for the tensor dimensions\n",
    "U, S, VT = tSVD(Fea_cube, rank)\n",
    "\n",
    "# Ensure shapes are compatible for tProduct\n",
    "print(f\"Shapes before tProduct: U={U.shape}, S={S.shape}, VT={VT.shape}\")\n",
    "assert U.shape[1] == S.shape[0]\n",
    "assert S.shape[1] == VT.shape[0]\n",
    "\n",
    "C = tProduct(U, S, VT)\n",
    "print(f\"Shape after first tProduct (C): {C.shape}\")\n",
    "\n",
    "# Perform the final tProduct\n",
    "Feacube_proc = C.reshape(L, W * H, B)\n",
    "print(f\"Shape after second tProduct (Feacube_proc): {Feacube_proc.shape}\")\n",
    "\n",
    "# SVM and RF based classification\n",
    "Labels = img_gt.reshape(W*H)\n",
    "Vectors = Feacube_proc.transpose(1, 0, 2).reshape(W*H, L*B)  # Use processed feature cube for SVM and RF\n",
    "\n",
    "class_num = np.max(img_gt) - np.min(img_gt)\n",
    "trainVectors, trainLabels, testVectors, testLabels = [], [], [], []\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "Sam = 0.02  # Training sample ratio: 0.02 for IP, 0.01 for PU and MG\n",
    "\n",
    "for k in range(1, class_num + 1):\n",
    "    index = np.where(Labels == k)[0]\n",
    "    perclass_num = len(index)\n",
    "    Vectors_perclass = Vectors[index, :]\n",
    "    c = rng.permutation(perclass_num)\n",
    "    select_train = Vectors_perclass[c[:int(np.ceil(perclass_num * Sam))], :]\n",
    "    train_index_k = index[c[:int(np.ceil(perclass_num * Sam))]]\n",
    "    trainVectors.append(select_train)\n",
    "    trainLabels.extend([k] * int(np.ceil(perclass_num * Sam)))\n",
    "\n",
    "    select_test = Vectors_perclass[c[int(np.ceil(perclass_num * Sam)):], :]\n",
    "    test_index_k = index[c[int(np.ceil(perclass_num * Sam)):]]\n",
    "    testVectors.append(select_test)\n",
    "    testLabels.extend([k] * (perclass_num - int(np.ceil(perclass_num * Sam))))\n",
    "\n",
    "trainVectors = np.vstack(trainVectors)\n",
    "testVectors = np.vstack(testVectors)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler().fit(trainVectors)\n",
    "trainVectors_scaled = scaler.transform(trainVectors)\n",
    "testVectors_scaled = scaler.transform(testVectors)\n",
    "Vectors_scaled = scaler.transform(Vectors)\n",
    "\n",
    "# SVM classification\n",
    "svm_classifier = SVC(kernel='linear', C=1)\n",
    "svm_classifier.fit(trainVectors_scaled, trainLabels)\n",
    "predicted_labels_svm = svm_classifier.predict(testVectors_scaled)\n",
    "\n",
    "# RF classification\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_classifier.fit(trainVectors_scaled, trainLabels)\n",
    "predicted_labels_rf = rf_classifier.predict(testVectors_scaled)\n",
    "\n",
    "# Evaluate the performance\n",
    "conf_matrix_svm = confusion_matrix(testLabels, predicted_labels_svm)\n",
    "accuracy_svm = accuracy_score(testLabels, predicted_labels_svm)\n",
    "kappa_svm = cohen_kappa_score(testLabels, predicted_labels_svm)\n",
    "\n",
    "conf_matrix_rf = confusion_matrix(testLabels, predicted_labels_rf)\n",
    "accuracy_rf = accuracy_score(testLabels, predicted_labels_rf)\n",
    "kappa_rf = cohen_kappa_score(testLabels, predicted_labels_rf)\n",
    "\n",
    "print(f\"SVM Confusion Matrix:\\n{conf_matrix_svm}\")\n",
    "print(f\"SVM Accuracy: {accuracy_svm}\")\n",
    "print(f\"SVM Kappa: {kappa_svm}\")\n",
    "\n",
    "print(f\"RF Confusion Matrix:\\n{conf_matrix_rf}\")\n",
    "print(f\"RF Accuracy: {accuracy_rf}\")\n",
    "print(f\"RF Kappa: {kappa_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec26c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading the file: Unable to open file (truncated file: eof = 46974730, sblock->base_addr = 512, stored_eof = 473815497)\n",
      "Using mock data for img since the file could not be loaded.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19932\\639236297.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m# TensorSSA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0mindian_pines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'symmetric'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m \u001b[0mId\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[0mFea_cube\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tSVD(tensor, rank):\n",
    "    \"\"\" Tensor Singular Value Decomposition \"\"\"\n",
    "    unfold_tensor = tensor.reshape(tensor.shape[0], -1)\n",
    "    U, S, VT = np.linalg.svd(unfold_tensor, full_matrices=False)\n",
    "    U = U[:, :rank]\n",
    "    S = np.diag(S)[:rank, :rank]\n",
    "    VT = VT[:rank, :]\n",
    "    return U, S, VT\n",
    "\n",
    "def tProduct(U, S, VT):\n",
    "    \"\"\" Tensor Product \"\"\"\n",
    "    result = np.tensordot(U, S, axes=(1, 0))\n",
    "    result = np.tensordot(result, VT, axes=(1, 0))\n",
    "    return result\n",
    "\n",
    "# Load the provided dataset\n",
    "file_path =  \"C:/Users/Mrlaptop/Downloads/Brain_Data_Mat_Format/Brain_Data_Mat_Format/008-01reflectance.mat\"\n",
    "\n",
    "\n",
    "data = {}\n",
    "try:\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Inspecting the keys to understand the structure of the data\n",
    "        print(f.keys())\n",
    "        # Assuming the dataset key is 'reflectance'\n",
    "        # Update this with the actual key after inspecting the structure\n",
    "        data = {key: np.array(f[key]) for key in f.keys()}\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the file: {e}\")\n",
    "\n",
    "# Ensure that the data was loaded\n",
    "if 'reflectance' in data:\n",
    "    img = data['reflectance']\n",
    "else:\n",
    "    print(\"Using mock data for img since the file could not be loaded.\")\n",
    "    img = np.random.rand(100, 100, 200)  # Mock data as an example\n",
    "    img_gt = np.random.randint(1, 5, (100, 100))  # Mock ground truth\n",
    "\n",
    "# If img_gt is not already defined (from the file), define a mock ground truth\n",
    "if 'img_gt' not in locals():\n",
    "    W, H, B = img.shape\n",
    "    img_gt = np.random.randint(1, 5, (W, H))  # Assuming there are 4 classes\n",
    "\n",
    "# Parameters (Can be changed as required)\n",
    "u = 5\n",
    "w = 2 * u + 1\n",
    "w2 = w * w\n",
    "L = 49\n",
    "\n",
    "# TensorSSA\n",
    "indian_pines = np.pad(img, ((u, u), (u, u), (0, 0)), mode='symmetric')\n",
    "Id = np.zeros((L, W * H), dtype=int)\n",
    "Fea_cube = np.zeros((L, W * H, B))\n",
    "\n",
    "# Adaptive embedding\n",
    "k = 0\n",
    "for i in range(W):\n",
    "    for j in range(H):\n",
    "        i1 = i + u\n",
    "        j1 = j + u\n",
    "        k += 1\n",
    "        testcube = indian_pines[i1-u:i1+u+1, j1-u:j1+u+1, :]\n",
    "        m = testcube.reshape(w2, B)\n",
    "\n",
    "        # NED\n",
    "        center = m[(w2+1)//2, :]\n",
    "        NED = np.sqrt(np.sum(((m / np.linalg.norm(m, axis=1, keepdims=True)) - (center / np.linalg.norm(center)))**2, axis=1))\n",
    "        ind = np.argsort(NED)\n",
    "        index = ind[:L]\n",
    "        Id[:, k-1] = index\n",
    "        Fea_cube[:, k-1, :] = m[index, :]\n",
    "\n",
    "# T-SVD decomposition\n",
    "rank = min(L, B)  # Ensure the rank is appropriate for the tensor dimensions\n",
    "U, S, VT = tSVD(Fea_cube, rank)\n",
    "\n",
    "# Ensure shapes are compatible for tProduct\n",
    "print(f\"Shapes before tProduct: U={U.shape}, S={S.shape}, VT={VT.shape}\")\n",
    "assert U.shape[1] == S.shape[0]\n",
    "assert S.shape[1] == VT.shape[0]\n",
    "\n",
    "C = tProduct(U, S, VT)\n",
    "print(f\"Shape after first tProduct (C): {C.shape}\")\n",
    "\n",
    "# Perform the final tProduct\n",
    "Feacube_proc = C.reshape(L, W * H, B)\n",
    "print(f\"Shape after second tProduct (Feacube_proc): {Feacube_proc.shape}\")\n",
    "\n",
    "# SVM and RF based classification\n",
    "Labels = img_gt.reshape(W*H)\n",
    "Vectors = Feacube_proc.transpose(1, 0, 2).reshape(W*H, L*B)  # Use processed feature cube for SVM and RF\n",
    "\n",
    "class_num = np.max(img_gt) - np.min(img_gt)\n",
    "trainVectors, trainLabels, testVectors, testLabels = [], [], [], []\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "Sam = 0.02  # Training sample ratio: 0.02 for IP, 0.01 for PU and MG\n",
    "\n",
    "for k in range(1, class_num + 1):\n",
    "    index = np.where(Labels == k)[0]\n",
    "    perclass_num = len(index)\n",
    "    Vectors_perclass = Vectors[index, :]\n",
    "    c = rng.permutation(perclass_num)\n",
    "    select_train = Vectors_perclass[c[:int(np.ceil(perclass_num * Sam))], :]\n",
    "    train_index_k = index[c[:int(np.ceil(perclass_num * Sam))]]\n",
    "    trainVectors.append(select_train)\n",
    "    trainLabels.extend([k] * int(np.ceil(perclass_num * Sam)))\n",
    "\n",
    "    select_test = Vectors_perclass[c[int(np.ceil(perclass_num * Sam)):], :]\n",
    "    test_index_k = index[c[int(np.ceil(perclass_num * Sam)):]]\n",
    "    testVectors.append(select_test)\n",
    "    testLabels.extend([k] * (perclass_num - int(np.ceil(perclass_num * Sam))))\n",
    "\n",
    "trainVectors = np.vstack(trainVectors)\n",
    "testVectors = np.vstack(testVectors)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler().fit(trainVectors)\n",
    "trainVectors_scaled = scaler.transform(trainVectors)\n",
    "testVectors_scaled = scaler.transform(testVectors)\n",
    "Vectors_scaled = scaler.transform(Vectors)\n",
    "\n",
    "# SVM classifier\n",
    "svm_model = SVC(kernel='linear', C=1)\n",
    "svm_model.fit(trainVectors_scaled, trainLabels)\n",
    "svm_predictions = svm_model.predict(testVectors_scaled)\n",
    "\n",
    "# Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(trainVectors_scaled, trainLabels)\n",
    "rf_predictions = rf_model.predict(testVectors_scaled)\n",
    "\n",
    "# Metrics\n",
    "svm_accuracy = accuracy_score(testLabels, svm_predictions)\n",
    "svm_kappa = cohen_kappa_score(testLabels, svm_predictions)\n",
    "rf_accuracy = accuracy_score(testLabels, rf_predictions)\n",
    "rf_kappa = cohen_kappa_score(testLabels, rf_predictions)\n",
    "\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "print(f\"SVM Kappa: {svm_kappa}\")\n",
    "print(f\"RF Accuracy: {rf_accuracy}\")\n",
    "print(f\"RF Kappa: {rf_kappa}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "svm_cm = confusion_matrix(testLabels, svm_predictions)\n",
    "rf_cm = confusion_matrix(testLabels, rf_predictions)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(svm_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[0].set_title('SVM Confusion Matrix')\n",
    "axes[1].imshow(rf_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[1].set_title('RF Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4add197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading the file: Unable to open file (truncated file: eof = 46974730, sblock->base_addr = 512, stored_eof = 473815497)\n",
      "Using mock data for img since the file could not be loaded.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19932\\19453853.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;31m# TensorSSA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[0mindian_pines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'symmetric'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0mId\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[0mFea_cube\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tSVD(tensor, rank):\n",
    "    \"\"\" Tensor Singular Value Decomposition \"\"\"\n",
    "    unfold_tensor = tensor.reshape(tensor.shape[0], -1)\n",
    "    U, S, VT = np.linalg.svd(unfold_tensor, full_matrices=False)\n",
    "    U = U[:, :rank]\n",
    "    S = np.diag(S)[:rank, :rank]\n",
    "    VT = VT[:rank, :]\n",
    "    return U, S, VT\n",
    "\n",
    "def tProduct(U, S, VT):\n",
    "    \"\"\" Tensor Product \"\"\"\n",
    "    result = np.tensordot(U, S, axes=(1, 0))\n",
    "    result = np.tensordot(result, VT, axes=(1, 0))\n",
    "    return result\n",
    "\n",
    "# Load the provided dataset\n",
    "file_path = \"C:/Users/Mrlaptop/Downloads/Brain_Data_Mat_Format/Brain_Data_Mat_Format/008-01reflectance.mat\"\n",
    "\n",
    "data = {}\n",
    "try:\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Inspecting the keys to understand the structure of the data\n",
    "        print(f.keys())\n",
    "        # Assuming the dataset key is 'reflectance'\n",
    "        # Update this with the actual key after inspecting the structure\n",
    "        data = {key: np.array(f[key]) for key in f.keys()}\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the file: {e}\")\n",
    "\n",
    "# Ensure that the data was loaded\n",
    "if 'reflectance' in data:\n",
    "    img = data['reflectance']\n",
    "else:\n",
    "    print(\"Using mock data for img since the file could not be loaded.\")\n",
    "    img = np.random.rand(100, 100, 200)  # Mock data as an example\n",
    "    img_gt = np.random.randint(1, 5, (100, 100))  # Mock ground truth\n",
    "\n",
    "# If img_gt is not already defined (from the file), define a mock ground truth\n",
    "if 'img_gt' not in locals():\n",
    "    W, H, B = img.shape\n",
    "    img_gt = np.random.randint(1, 5, (W, H))  # Assuming there are 4 classes\n",
    "\n",
    "# Parameters (Can be changed as required)\n",
    "u = 5\n",
    "w = 2 * u + 1\n",
    "w2 = w * w\n",
    "L = 49\n",
    "\n",
    "# TensorSSA\n",
    "indian_pines = np.pad(img, ((u, u), (u, u), (0, 0)), mode='symmetric')\n",
    "Id = np.zeros((L, W * H), dtype=int)\n",
    "Fea_cube = np.zeros((L, W * H, B))\n",
    "\n",
    "# Adaptive embedding\n",
    "k = 0\n",
    "for i in range(W):\n",
    "    for j in range(H):\n",
    "        i1 = i + u\n",
    "        j1 = j + u\n",
    "        k += 1\n",
    "        testcube = indian_pines[i1-u:i1+u+1, j1-u:j1+u+1, :]\n",
    "        m = testcube.reshape(w2, B)\n",
    "\n",
    "        # NED\n",
    "        center = m[(w2+1)//2, :]\n",
    "        NED = np.sqrt(np.sum(((m / np.linalg.norm(m, axis=1, keepdims=True)) - (center / np.linalg.norm(center)))**2, axis=1))\n",
    "        ind = np.argsort(NED)\n",
    "        index = ind[:L]\n",
    "        Id[:, k-1] = index\n",
    "        Fea_cube[:, k-1, :] = m[index, :]\n",
    "\n",
    "# T-SVD decomposition\n",
    "rank = min(L, B)  # Ensure the rank is appropriate for the tensor dimensions\n",
    "U, S, VT = tSVD(Fea_cube, rank)\n",
    "\n",
    "# Ensure shapes are compatible for tProduct\n",
    "print(f\"Shapes before tProduct: U={U.shape}, S={S.shape}, VT={VT.shape}\")\n",
    "assert U.shape[1] == S.shape[0]\n",
    "assert S.shape[1] == VT.shape[0]\n",
    "\n",
    "C = tProduct(U, S, VT)\n",
    "print(f\"Shape after first tProduct (C): {C.shape}\")\n",
    "\n",
    "# Perform the final tProduct\n",
    "Feacube_proc = C.reshape(L, W * H, B)\n",
    "print(f\"Shape after second tProduct (Feacube_proc): {Feacube_proc.shape}\")\n",
    "\n",
    "# SVM and RF based classification\n",
    "Labels = img_gt.reshape(W*H)\n",
    "Vectors = Feacube_proc.transpose(1, 0, 2).reshape(W*H, L*B)  # Use processed feature cube for SVM and RF\n",
    "\n",
    "class_num = np.max(img_gt) - np.min(img_gt)\n",
    "trainVectors, trainLabels, testVectors, testLabels = [], [], [], []\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "Sam = 0.02  # Training sample ratio: 0.02 for IP, 0.01 for PU and MG\n",
    "\n",
    "for k in range(1, class_num + 1):\n",
    "    index = np.where(Labels == k)[0]\n",
    "    perclass_num = len(index)\n",
    "    Vectors_perclass = Vectors[index, :]\n",
    "    c = rng.permutation(perclass_num)\n",
    "    select_train = Vectors_perclass[c[:int(np.ceil(perclass_num * Sam))], :]\n",
    "    train_index_k = index[c[:int(np.ceil(perclass_num * Sam))]]\n",
    "    trainVectors.append(select_train)\n",
    "    trainLabels.extend([k] * int(np.ceil(perclass_num * Sam)))\n",
    "\n",
    "    select_test = Vectors_perclass[c[int(np.ceil(perclass_num * Sam)):], :]\n",
    "    test_index_k = index[c[int(np.ceil(perclass_num * Sam)):]]\n",
    "    testVectors.append(select_test)\n",
    "    testLabels.extend([k] * (perclass_num - int(np.ceil(perclass_num * Sam))))\n",
    "\n",
    "trainVectors = np.vstack(trainVectors)\n",
    "testVectors = np.vstack(testVectors)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler().fit(trainVectors)\n",
    "trainVectors_scaled = scaler.transform(trainVectors)\n",
    "testVectors_scaled = scaler.transform(testVectors)\n",
    "Vectors_scaled = scaler.transform(Vectors)\n",
    "\n",
    "# SVM classifier\n",
    "svm_model = SVC(kernel='linear', C=1)\n",
    "svm_model.fit(trainVectors_scaled, trainLabels)\n",
    "svm_predictions = svm_model.predict(testVectors_scaled)\n",
    "\n",
    "# Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(trainVectors_scaled, trainLabels)\n",
    "rf_predictions = rf_model.predict(testVectors_scaled)\n",
    "\n",
    "# Metrics\n",
    "svm_accuracy = accuracy_score(testLabels, svm_predictions)\n",
    "svm_kappa = cohen_kappa_score(testLabels, svm_predictions)\n",
    "rf_accuracy = accuracy_score(testLabels, rf_predictions)\n",
    "rf_kappa = cohen_kappa_score(testLabels, rf_predictions)\n",
    "\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "print(f\"SVM Kappa: {svm_kappa}\")\n",
    "print(f\"RF Accuracy: {rf_accuracy}\")\n",
    "print(f\"RF Kappa: {rf_kappa}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "svm_cm = confusion_matrix(testLabels, svm_predictions)\n",
    "rf_cm = confusion_matrix(testLabels, rf_predictions)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(svm_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[0].set_title('SVM Confusion Matrix')\n",
    "axes[1].imshow(rf_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[1].set_title('RF Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c745f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['gt', 'refl']>\n",
      "Unable to load the dataset.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12720\\1710037254.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# Mock ground truth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0mimg_gt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Assuming there are 4 classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tSVD(tensor, rank):\n",
    "    \"\"\" Tensor Singular Value Decomposition \"\"\"\n",
    "    unfold_tensor = tensor.reshape(tensor.shape[0], -1)\n",
    "    U, S, VT = np.linalg.svd(unfold_tensor, full_matrices=False)\n",
    "    U = U[:, :rank]\n",
    "    S = np.diag(S)[:rank, :rank]\n",
    "    VT = VT[:rank, :]\n",
    "    return U, S, VT\n",
    "\n",
    "def tProduct(U, S, VT):\n",
    "    \"\"\" Tensor Product \"\"\"\n",
    "    result = np.tensordot(U, S, axes=(1, 0))\n",
    "    result = np.tensordot(result, VT, axes=(1, 0))\n",
    "    return result\n",
    "\n",
    "# Load the provided dataset\n",
    "file_path =\"C:/Users/Mrlaptop/Downloads/Brain_Data_Mat_Format/Brain_Data_Mat_Format/004-02reflectance.mat\"\n",
    "\n",
    "data = {}\n",
    "try:\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # Inspecting the keys to understand the structure of the data\n",
    "        print(f.keys())\n",
    "        # Assuming the dataset key is 'reflectance'\n",
    "        data = {key: np.array(f[key]) for key in f.keys()}\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the file: {e}\")\n",
    "\n",
    "# Ensure that the data was loaded\n",
    "if 'reflectance' in data:\n",
    "    img = data['reflectance']\n",
    "else:\n",
    "    print(\"Unable to load the dataset.\")\n",
    "    exit()\n",
    "\n",
    "# Mock ground truth\n",
    "W, H, B = img.shape\n",
    "img_gt = np.random.randint(1, 5, (W, H))  # Assuming there are 4 classes\n",
    "\n",
    "# Parameters (Can be changed as required)\n",
    "u = 5\n",
    "w = 2 * u + 1\n",
    "w2 = w * w\n",
    "L = 49\n",
    "\n",
    "# TensorSSA\n",
    "indian_pines = np.pad(img, ((u, u), (u, u), (0, 0)), mode='symmetric')\n",
    "Id = np.zeros((L, W * H), dtype=int)\n",
    "Fea_cube = np.zeros((L, W * H, B))\n",
    "\n",
    "# Adaptive embedding\n",
    "k = 0\n",
    "for i in range(W):\n",
    "    for j in range(H):\n",
    "        i1 = i + u\n",
    "        j1 = j + u\n",
    "        k += 1\n",
    "        testcube = indian_pines[i1-u:i1+u+1, j1-u:j1+u+1, :]\n",
    "        m = testcube.reshape(w2, B)\n",
    "\n",
    "        # NED\n",
    "        center = m[(w2+1)//2, :]\n",
    "        NED = np.sqrt(np.sum(((m / np.linalg.norm(m, axis=1, keepdims=True)) - (center / np.linalg.norm(center)))**2, axis=1))\n",
    "        ind = np.argsort(NED)\n",
    "        index = ind[:L]\n",
    "        Id[:, k-1] = index\n",
    "        Fea_cube[:, k-1, :] = m[index, :]\n",
    "\n",
    "# T-SVD decomposition\n",
    "rank = min(L, B)  # Ensure the rank is appropriate for the tensor dimensions\n",
    "U, S, VT = tSVD(Fea_cube, rank)\n",
    "\n",
    "# Ensure shapes are compatible for tProduct\n",
    "print(f\"Shapes before tProduct: U={U.shape}, S={S.shape}, VT={VT.shape}\")\n",
    "assert U.shape[1] == S.shape[0]\n",
    "assert S.shape[1] == VT.shape[0]\n",
    "\n",
    "C = tProduct(U, S, VT)\n",
    "print(f\"Shape after first tProduct (C): {C.shape}\")\n",
    "\n",
    "# Perform the final tProduct\n",
    "Feacube_proc = C.reshape(L, W * H, B)\n",
    "print(f\"Shape after second tProduct (Feacube_proc): {Feacube_proc.shape}\")\n",
    "\n",
    "# SVM and RF based classification\n",
    "Labels = img_gt.reshape(W*H)\n",
    "Vectors = Feacube_proc.transpose(1, 0, 2).reshape(W*H, L*B)  # Use processed feature cube for SVM and RF\n",
    "\n",
    "class_num = np.max(img_gt) - np.min(img_gt)\n",
    "trainVectors, trainLabels, testVectors, testLabels = [], [], [], []\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "Sam = 0.02  # Training sample ratio: 0.02 for IP, 0.01 for PU and MG\n",
    "\n",
    "for k in range(1, class_num + 1):\n",
    "    index = np.where(Labels == k)[0]\n",
    "    perclass_num = len(index)\n",
    "    Vectors_perclass = Vectors[index, :]\n",
    "    c = rng.permutation(perclass_num)\n",
    "    select_train = Vectors_perclass[c[:int(np.ceil(perclass_num * Sam))], :]\n",
    "    train_index_k = index[c[:int(np.ceil(perclass_num * Sam))]]\n",
    "    trainVectors.append(select_train)\n",
    "    trainLabels.extend([k] * int(np.ceil(perclass_num * Sam)))\n",
    "\n",
    "    select_test = Vectors_perclass[c[int(np.ceil(perclass_num * Sam)):], :]\n",
    "    test_index_k = index[c[int(np.ceil(perclass_num * Sam)):]]\n",
    "    testVectors.append(select_test)\n",
    "    testLabels.extend([k] * (perclass_num - int(np.ceil(perclass_num * Sam))))\n",
    "\n",
    "trainVectors = np.vstack(trainVectors)\n",
    "testVectors = np.vstack(testVectors)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler().fit(trainVectors)\n",
    "trainVectors_scaled = scaler.transform(trainVectors)\n",
    "testVectors_scaled = scaler.transform(testVectors)\n",
    "Vectors_scaled = scaler.transform(Vectors)\n",
    "\n",
    "# SVM classifier\n",
    "svm_model = SVC(kernel='linear', C=1)\n",
    "svm_model.fit(trainVectors_scaled, trainLabels)\n",
    "svm_predictions = svm_model.predict(testVectors_scaled)\n",
    "\n",
    "# Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(trainVectors_scaled, trainLabels)\n",
    "rf_predictions = rf_model.predict(testVectors_scaled)\n",
    "\n",
    "# Metrics\n",
    "svm_accuracy = accuracy_score(testLabels, svm_predictions)\n",
    "svm_kappa = cohen_kappa_score(testLabels, svm_predictions)\n",
    "rf_accuracy = accuracy_score(testLabels, rf_predictions)\n",
    "rf_kappa = cohen_kappa_score(testLabels, rf_predictions)\n",
    "\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "print(f\"SVM Kappa: {svm_kappa}\")\n",
    "print(f\"RF Accuracy: {rf_accuracy}\")\n",
    "print(f\"RF Kappa: {rf_kappa}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "svm_cm = confusion_matrix(testLabels, svm_predictions)\n",
    "rf_cm = confusion_matrix(testLabels, rf_predictions)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(svm_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[0].set_title('SVM Confusion Matrix')\n",
    "axes[1].imshow(rf_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "axes[1].set_title('RF Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f96309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
